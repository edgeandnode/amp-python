import decimal
import json
import os
import os.path
import time
from collections import defaultdict
from typing import Union, Iterator
import base58
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

import requests
from eth_utils import keccak
from google.cloud import bigquery, storage


def process_and_save_dataframe(final_df: pd.DataFrame, output_directory: str, file_name: str = 'final_df.parquet'):
    # Ensure the output directory exists and is writable
    if not os.access(output_directory, os.W_OK):
        raise OSError(f'Cannot write to the specified directory: {output_directory}')

    # Convert the defaultdict to a regular dictionary if needed
    if isinstance(final_df, defaultdict):
        indexer_dict = {key: dict(value) for key, value in final_df.items()}
        final_df = pd.DataFrame.from_dict(indexer_dict, orient='index')
        final_df.reset_index(inplace=True)

    # Ensure the date column is in datetime format
    if 'date' in final_df.columns:
        final_df['date'] = pd.to_datetime(final_df['date'], errors='coerce')
        final_df['date'] = final_df['date'].dt.date  # Convert to date format

    # Define schema using pyarrow to ensure compatibility
    schema = pa.schema(
        [
            ('date', pa.date32()),
            ('id', pa.string()),
            ('value', pa.float64()),  # Define the decimal type to match BigQuery's NUMERIC type
        ]
    )

    # Convert pandas DataFrame to pyarrow Table
    table = pa.Table.from_pandas(final_df, schema=schema)

    # Construct the full file path
    file_path = os.path.join(output_directory, file_name)

    # Save the Table to a Parquet file
    pq.write_table(table, file_path)

    print(f'File saved successfully at {file_path}')


def generate_signatures_from_abi(file_path, specific_event_name=None):
    # Read the ABI file
    with open(file_path, 'r') as file:
        abi = json.load(file)

    # Initialize a list to store signatures
    signatures = []

    # Iterate over each item in the ABI
    for item in abi:
        # Check if the item is an event
        if item['type'] == 'event':
            # Extract the event name
            event_name = item['name']

            # Extract the inputs and construct the input part of the signature
            inputs = item['inputs']
            inputs_signature = ', '.join(
                f'{input["type"]} {"indexed " if input["indexed"] else ""}{input["name"]}' for input in inputs
            )

            # Construct the full signature
            signature = f'{event_name}({inputs_signature})'

            # If specific_event_name is given, check if it matches the current event name
            if specific_event_name:
                if event_name == specific_event_name:
                    signatures.append(signature)
            else:
                # Add the signature to the list
                signatures.append(signature)
    print(signatures)
    return signatures


def generate_signature(json_string):
    # Parse the JSON string
    data = json.loads(json_string)

    # Extract the event name
    event_name = data['name']

    # Extract the inputs and construct the input part of the signature
    inputs = data['inputs']
    inputs_signature = ', '.join(
        f'{input["type"]} {"indexed " if input["indexed"] else ""}{input["name"]}' for input in inputs
    )

    # Construct the full signature
    signature = f'{event_name}({inputs_signature})'

    return signature


def elapsed(start):
    return round(time.time() - start, 4)


def process_query(client, query):
    # df = pd.DataFrame()

    print(query)
    start = time.time()

    result_stream = client.get_sql(query)
    print('time to establish stream: ', elapsed(start), 's')

    total_events = 0
    try:
        batch = next(result_stream)
        print('time for first batch ', elapsed(start), 's')
        total_events += batch.num_rows
        df = batch.to_pandas().map(to_hex)

        print('The type of df is ', type(df))

        batch_start = time.time()

        for batch in result_stream:
            total_events += batch.num_rows
            print('received batch of ', batch.num_rows, ' rows in ', elapsed(batch_start), 's')
            batch_start = time.time()
            new_df = batch.to_pandas().map(to_hex)
            # Concatenate the df dataframe to the previous dataframe
            df = pd.concat([df, new_df])

        print('total rows: ', total_events)
        print('total time to consume the stream: ', elapsed(start), 's')
        print('rows/s: ', total_events / elapsed(start))
        print('Here are some data ', df.head())
        return df

    except StopIteration:
        print('No more batches available in the result stream.')


def convert_bigint_subgraph_id_to_base58(bigint_representation: int) -> str:
    # Convert the bigint to a hex string
    hex_string = hex(bigint_representation)

    # Check if the hex string length is even
    if len(hex_string) % 2 != 0:
        # Log a warning if the hex string length is not even and pad it to even length
        print(
            f'Warning: Hex string not even, padding it to even length'
            f'  hex: {hex_string}, '
            f'  original: {bigint_representation}. '
        )
        hex_string = '0x0' + hex_string[2:]

    # Convert the hex string to a byte array
    bytes_data = bytes.fromhex(hex_string[2:])

    # Convert the byte array to a Base58 string
    base58_string = base58.b58encode(bytes_data).decode('utf-8')

    return base58_string


def convert_to_base58(subgraph_metadata_hex):
    print(subgraph_metadata_hex)
    subgraph_metadata_hex = subgraph_metadata_hex[2:]
    # Convert hex string to bytes
    subgraph_metadata_bytes = bytes.fromhex(subgraph_metadata_hex)

    # Add the prefix using add_qm function
    prefixed_bytes = add_qm(subgraph_metadata_bytes)

    # Convert bytes to Base58
    base58_string = base58.b58encode(prefixed_bytes).decode('utf-8')

    return base58_string


def add_qm(byte_array):
    # Create an output bytearray with a size of 34
    out = bytearray(34)

    # Set the first two bytes
    out[0] = 0x12
    out[1] = 0x20

    # Copy the input byte array into the output starting from the third byte
    for i in range(32):
        out[i + 2] = byte_array[i]

    return bytes(out)


def get_subgraph_id(graphAccount, subgraphNumber):
    # Step 1: Remove '0x' prefix if present and convert graphAccount to bytes
    if isinstance(graphAccount, str):
        if graphAccount.startswith('0x'):
            graphAccount = graphAccount[2:]
        graphAccount = bytes.fromhex(graphAccount)

    # Step 2: Convert graphAccount to a hexadecimal string
    graphAccountStr = graphAccount.hex()

    # Step 3: Convert subgraphNumber to an integer if it's a decimal.Decimal, then to a hexadecimal string
    if isinstance(subgraphNumber, decimal.Decimal):
        subgraphNumber = int(subgraphNumber)

    subgraphNumberStr = hex(subgraphNumber)[2:].zfill(64)

    # Step 4: Concatenate the two strings
    unhashedSubgraphID = graphAccountStr + subgraphNumberStr

    # Step 5: Hash the concatenated string using Keccak256
    hashedId = keccak(bytes.fromhex(unhashedSubgraphID))

    # Step 6: Convert the hash to a BigInt
    bigIntRepresentation = int.from_bytes(hashedId, byteorder='big')

    return convert_bigint_subgraph_id_to_base58(bigIntRepresentation)


def export_df_to_csv(path, df):
    csv_file_path = path  # Replace with your desired file path
    df.to_csv(csv_file_path, index=False)


def fetch_ipfs_data(url):
    try:
        # Fetch data from IPFS using the provided URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for HTTP errors
        return response.text
    except requests.RequestException as e:
        print(f'Error fetching IPFS data: {e}')
        return None


def save_json_file(data, filename):
    try:
        with open(filename, 'w') as file:
            json.dump(data, file, indent=4)
    except Exception as e:
        print(f'Error saving JSON file: {e}')


def json_to_string(value):
    return str(value) if value else ''


def process_subgraph_metadata(json_data):
    display_name = json_data['displayName']
    code_repository = json_data['codeRepository']
    website = json_data['website']
    return display_name, code_repository, website


def get_ipfs_data(df):
    for ipfs_hash in df['metadata_ipfs_hash']:
        ipfs_url = 'https://ipfs.thegraph.com/ipfs/' + ipfs_hash
        ipfs_data = fetch_ipfs_data(ipfs_url)
        if ipfs_data:
            json_data = json.loads(ipfs_data)  # Convert the fetched data to a JSON object
        # Process the subgraph metadata
        df['display_name'] = json_data['displayName']
        df['code_repository'] = json_data['codeRepository']
        df['website'] = json_data['website']


def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):
    """Uploads a file to the bucket."""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    blob.upload_from_filename(source_file_name)

    print(f'File {source_file_name} uploaded to {destination_blob_name}.')


def load_parquet_to_bigquery(dataset_id, table_id, gcs_uri):
    """Loads a Parquet file from GCS into BigQuery."""
    client = bigquery.Client()

    table_ref = client.dataset(dataset_id).table(table_id)
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.PARQUET,
    )

    load_job = client.load_table_from_uri(gcs_uri, table_ref, job_config=job_config)

    print(f'Starting job {load_job.job_id}')

    load_job.result()  # Waits for the job to complete.

    print(f'Job finished. Loaded {load_job.output_rows} rows into {dataset_id}:{table_id}')


# Convert bytes columns to hex
def to_hex(val):
    return '0x' + val.hex() if isinstance(val, bytes) else val


class Abi:
    def __init__(self, path):
        f = open(path)
        data = json.load(f)
        self.events = {}
        for entry in data:
            if entry['type'] == 'event':
                self.events[entry['name']] = Event(entry)


# An event from a JSON ABI
class Event:
    def __init__(self, data):
        self.name = data['name']
        self.inputs = []
        self.names = []
        for input in data['inputs']:
            param = input['type']
            self.names.append(input['name'])
            if input['indexed']:
                param += ' indexed'
            param += ' ' + input['name']
            self.inputs.append(param)

    def signature(self):
        sig = self.name + '(' + ','.join(self.inputs) + ')'
        return sig

def decode_arrow_data(data: Union[pa.Table, pa.RecordBatch, Iterator[pa.RecordBatch]]) -> Union[pa.Table, pa.RecordBatch, Iterator[pa.RecordBatch]]:
    """
    Decode bytes-like columns in Arrow data to '0x'-prefixed hex strings using to_hex.
    
    Handles standard binary, fixed-size binary, and custom/extension types that contain bytes.
    Dynamically updates the schema for decoded columns to string type.
    Preserves the input type (Table -> Table, RecordBatch -> RecordBatch, iterator -> iterator).
    
    Args:
        data: Arrow Table, RecordBatch, or iterator of RecordBatches.
    
    Returns: Decoded data in the same format/type as input.
    """
    def decode_batch(batch: pa.RecordBatch) -> pa.RecordBatch:
        new_arrays = []
        new_fields = []  # Build new schema fields
        for field, array in zip(batch.schema, batch):
            # Broader check for any binary-like type (standard, fixed-size, or custom/extension)
            if pa.types.is_binary(field.type) or pa.types.is_large_binary(field.type) or pa.types.is_fixed_size_binary(field.type):
                # Apply to_hex to each value (handles nulls and converts to hex string)
                decoded = [to_hex(val.as_py()) if val is not None else None for val in array]
                new_arrays.append(pa.array(decoded, type=pa.string()))
                # Update field to string type in new schema
                new_fields.append(field.with_type(pa.string()))
            else:
                new_arrays.append(array)
                new_fields.append(field)  # Keep original
        # Create new schema from updated fields
        new_schema = pa.schema(new_fields)
        return pa.RecordBatch.from_arrays(new_arrays, schema=new_schema)

    if isinstance(data, pa.Table):
        # Decode as batches and reconstruct Table with new schema
        batches = [decode_batch(b) for b in data.to_batches()]
        if batches:
            return pa.Table.from_batches(batches, schema=batches[0].schema)
        else:
            return pa.Table.from_batches([], schema=data.schema)  # Handle empty
    elif isinstance(data, pa.RecordBatch):
        return decode_batch(data)
    elif hasattr(data, '__iter__'):  # Iterator of batches
        def generator():
            for batch in data:
                yield decode_batch(batch)
        return generator()
    else:
        raise ValueError(f"Unsupported data type: {type(data)}. Expected Table, RecordBatch, or iterator.")